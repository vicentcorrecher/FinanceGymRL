{"cells":[{"cell_type":"markdown","source":["# StockTradingEnv"],"metadata":{"id":"rWOgNUvHjwJi"}},{"cell_type":"markdown","metadata":{"id":"l2CY6oSzxG0M"},"source":["\n","**Características del environment**\n","\n","StockTradingEnv es un entorno de trading de acciones para OpenAI Gym, donde un agente toma decisiones de compra y venta de acciones. Las características del environment son las siguientes:\n","\n","\n","**Tipos de estados**\n","\n","Espacio de Estados:\n","El espacio de estados es continuo y multidimensional, compuesto por:\n","\n","*   Posición de apertura de las acciones de los últimos cinco días, escalada entre 0 y 1.\n","* Máximo precio de las acciones de los últimos cinco días, escalada entre 0 y 1.\n","* Mínimo precio de las acciones de los últimos cinco días, escalada entre 0 y 1.\n","* Precio de cierre de las acciones de los últimos cinco días, escalada entre 0 y 1.\n","* Volumen de las acciones de los últimos cinco días, escalado entre 0 y 1.\n","* Datos adicionales escalados entre 0 y 1:\n","  - Saldo actual (balance).\n","  - Valor neto máximo alcanzado (max_net_worth).\n","  - Acciones en posesión (shares_held).\n","  - Base de coste media de las acciones en posesión (cost_basis).\n","  - Total de acciones vendidas (total_shares_sold).\n","  - Valor total de las ventas (total_sales_value).\n","\n","**Tipos de acciones**\n","\n","Espacio de Acciones:\n","El espacio de acciones es continuo, con dos dimensiones:\n","\n","- Tipo de acción (0: Comprar, 1: Vender, 2: Mantener).\n","- Cantidad de la acción (proporción del saldo o de las acciones en posesión).\n","\n","**Recompensas**\n","\n","La recompensa se calcula en función del saldo actual multiplicado por un modificador de retraso, que es proporcional al progreso dentro del episodio.\n","No hay recompensas positivas o adicionales explícitas por alcanzar ciertos objetivos, pero la recompensa implícita es maximizar el valor neto de la cartera a lo largo del tiempo.\n","\n","Recompensa media esperada\n","En StockTradingEnv, el objetivo es que el agente aprenda a maximizar el valor neto de su cartera a lo largo del tiempo mediante decisiones de compra y venta de acciones. La recompensa promedio esperada varía según el rendimiento del agente, pero un agente bien entrenado debería ser capaz de aumentar su valor neto de manera consistente, minimizando pérdidas y aprovechando oportunidades de ganancia."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OiDQGnzAUYDV","executionInfo":{"status":"ok","timestamp":1719571762311,"user_tz":-120,"elapsed":87669,"user":{"displayName":"Vicent","userId":"09438679983193344385"}},"outputId":"c91e80c5-ac05-481d-edd8-cac42f359bba"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q stable_baselines3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2791JsK9-JB"},"outputs":[],"source":["import json\n","import datetime as dt\n","import random\n","import json\n","import gym\n","from gym import spaces\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.callbacks import EvalCallback"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_d1SlJQb5mw"},"outputs":[],"source":["# Definición de constantes\n","MAX_ACCOUNT_BALANCE = 2147483647\n","MAX_NUM_SHARES = 2147483647\n","MAX_SHARE_PRICE = 5000\n","MAX_OPEN_POSITIONS = 5\n","MAX_STEPS = 20000\n","INITIAL_ACCOUNT_BALANCE = 10000\n","\n","class StockTradingEnv(gym.Env):\n","    \"\"\"Un entorno de trading de acciones para OpenAI gym\"\"\"\n","    metadata = {'render.modes': ['human']}\n","\n","    def __init__(self, df):\n","        super(StockTradingEnv, self).__init__()\n","\n","        self.df = df\n","        self.reward_range = (0, MAX_ACCOUNT_BALANCE)\n","\n","        # Espacio de acciones: [acción, cantidad]\n","        # acción: 0 = mantener, 1 = comprar, 2 = vender\n","        # cantidad: 0 a 1 (porcentaje del saldo/acciones en posesión)\n","        self.action_space = spaces.Box(\n","            low=np.array([0, 0], dtype=np.float32),\n","            high=np.array([2, 1], dtype=np.float32),\n","            dtype=np.float32\n","        )\n","\n","        # Espacio de observación: contiene los valores OHLC de los últimos cinco días y otras características\n","        self.observation_space = spaces.Box(\n","            low=0, high=1, shape=(6, 6), dtype=np.float32)\n","\n","    def _next_observation(self):\n","        # Obtener los puntos de datos de acciones de los últimos 5 días y escalarlos entre 0-1\n","        frame = np.array([\n","            self.df.loc[self.current_step: self.current_step + 5, 'Open'].values / MAX_SHARE_PRICE,\n","            self.df.loc[self.current_step: self.current_step + 5, 'High'].values / MAX_SHARE_PRICE,\n","            self.df.loc[self.current_step: self.current_step + 5, 'Low'].values / MAX_SHARE_PRICE,\n","            self.df.loc[self.current_step: self.current_step + 5, 'Close'].values / MAX_SHARE_PRICE,\n","            self.df.loc[self.current_step: self.current_step + 5, 'Volume'].values / MAX_NUM_SHARES,\n","        ], dtype=np.float32)\n","\n","        # Agregar datos adicionales y escalar cada valor entre 0-1\n","        obs = np.append(frame, [[\n","            self.balance / MAX_ACCOUNT_BALANCE,\n","            self.max_net_worth / MAX_ACCOUNT_BALANCE,\n","            self.shares_held / MAX_NUM_SHARES,\n","            self.cost_basis / MAX_SHARE_PRICE,\n","            self.total_shares_sold / MAX_NUM_SHARES,\n","            self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE),\n","        ]], axis=0)\n","\n","        return obs\n","\n","    def _take_action(self, action):\n","        # Escalar las acciones al rango esperado\n","        action_type = int(action[0])  # 0 = mantener, 1 = comprar, 2 = vender\n","        amount = action[1]  # Esto ya está en el rango [0, 1]\n","\n","        # Establecer el precio actual a un precio aleatorio dentro del intervalo de tiempo\n","        current_price = random.uniform(\n","            self.df.loc[self.current_step, \"Open\"], self.df.loc[self.current_step, \"Close\"])\n","\n","        if action_type == 1:\n","            # Comprar amount % del saldo en acciones\n","            total_possible = int(self.balance / current_price)\n","            shares_bought = int(total_possible * amount)\n","            prev_cost = self.cost_basis * self.shares_held\n","            additional_cost = shares_bought * current_price\n","\n","            self.balance -= additional_cost\n","            self.cost_basis = (prev_cost + additional_cost) / (self.shares_held + shares_bought)\n","            self.shares_held += shares_bought\n","\n","        elif action_type == 2:\n","            # Vender amount % de las acciones en posesión\n","            shares_sold = int(self.shares_held * amount)\n","            self.balance += shares_sold * current_price\n","            self.shares_held -= shares_sold\n","            self.total_shares_sold += shares_sold\n","            self.total_sales_value += shares_sold * current_price\n","\n","        self.net_worth = self.balance + self.shares_held * current_price\n","\n","        if self.net_worth > self.max_net_worth:\n","            self.max_net_worth = self.net_worth\n","\n","        if self.shares_held == 0:\n","            self.cost_basis = 0\n","\n","    def step(self, action):\n","        # Ejecutar un paso dentro del entorno\n","        self._take_action(action)\n","\n","        self.current_step += 1\n","\n","        if self.current_step > len(self.df.loc[:, 'Open'].values) - 6:\n","            self.current_step = 0\n","\n","        delay_modifier = (self.current_step / MAX_STEPS)\n","\n","        # Calcular la recompensa basada en el cambio de valor neto\n","        reward = self.net_worth * delay_modifier\n","        done = self.net_worth <= 0\n","\n","        obs = self._next_observation()\n","\n","        return obs, reward, done, {}\n","\n","    def reset(self):\n","        # Reiniciar el estado del entorno a un estado inicial\n","        self.balance = INITIAL_ACCOUNT_BALANCE\n","        self.net_worth = INITIAL_ACCOUNT_BALANCE\n","        self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n","        self.shares_held = 0\n","        self.cost_basis = 0\n","        self.total_shares_sold = 0\n","        self.total_sales_value = 0\n","\n","        # Establecer el paso actual en un punto aleatorio dentro del marco de datos\n","        self.current_step = random.randint(0, len(self.df.loc[:, 'Open'].values) - 6)\n","\n","        return self._next_observation()\n","\n","    def render(self, mode='human', close=False):\n","        profit = self.net_worth - INITIAL_ACCOUNT_BALANCE\n","\n","        print(f'Step: {self.current_step}')\n","        print(f'Balance: {self.balance}')\n","        print(f'Shares held: {self.shares_held} (Total sold: {self.total_shares_sold})')\n","        print(f'Avg cost for held shares: {self.cost_basis} (Total sales value: {self.total_sales_value})')\n","        print(f'Net worth: {self.net_worth} (Max net worth: {self.max_net_worth})')\n","        print(f'Profit: {profit}')\n","\n","        plt.figure(figsize=(10, 6))\n","        plt.subplot(2, 1, 1)\n","        plt.plot(self.df.loc[:self.current_step, 'Close'], label='Close Price')\n","        plt.title('Stock Price Over Time')\n","        plt.xlabel('Time')\n","        plt.ylabel('Price')\n","        plt.legend()\n","\n","        plt.subplot(2, 1, 2)\n","        plt.plot([INITIAL_ACCOUNT_BALANCE] + [self.net_worth], label='Net Worth')\n","        plt.title('Net Worth Over Time')\n","        plt.xlabel('Time')\n","        plt.ylabel('Net Worth')\n","        plt.legend()\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iBTreqTCyP3A"},"source":["Trabajamos con datos históricos de acciones de Apple (AAPL).\n","\n","---\n","\n","\n","El DataFrame contiene un total de 5255 filas y 8 columnas.\n","Las columnas incluyen información sobre el índice, la fecha, los precios de apertura, alto, bajo y cierre, así como el volumen de transacciones.\n","Los datos están ordenados cronológicamente por fecha.\n","Estadísticas Resumidas:\n","\n","Los precios de apertura, alto, bajo y cierre tienen una amplia gama de valores, con desviaciones estándar significativas, lo que indica una gran variabilidad en los precios a lo largo del tiempo.\n","El volumen de transacciones también varía considerablemente, con un rango que va desde cientos de miles hasta casi 200 millones.\n","Tipos de Datos:\n","La mayoría de las columnas contienen datos numéricos, con la excepción de la columna \"Date\", que parece contener fechas en formato de objeto.\n","Es importante convertir la columna \"Date\" a un tipo de dato de fecha y hora para facilitar su manipulación y análisis temporal."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOAZGPlonYA9"},"outputs":[],"source":["df = pd.read_csv('/content/drive/MyDrive/Master IA^3/Reinforcement Learning/Proyecto RL/Custom Trading Environment/Data/AAPL.csv')\n","df = df.sort_values('Date').reset_index(drop=True)\n","df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']].dropna()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1719571901139,"user":{"displayName":"Vicent","userId":"09438679983193344385"},"user_tz":-120},"id":"JYJuEthOx3UV","outputId":"518183bf-3108-4441-ed02-4f20f47ba8f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Primeras filas del dataframe:\n","   index  Unnamed: 0        Date   Open   High    Low  Close      Volume\n","0      0           0  1998-01-02  13.63  16.25  13.50  16.25   6411700.0\n","1      1           1  1998-01-05  16.50  16.56  15.19  15.88   5820300.0\n","2      2           2  1998-01-06  15.94  20.00  14.75  18.94  16182800.0\n","3      3           3  1998-01-07  18.81  19.00  17.31  17.50   9300200.0\n","4      4           4  1998-01-08  17.44  18.62  16.94  18.19   6910900.0\n","\n","Estadísticas resumidas del dataframe:\n","             index   Unnamed: 0         Open         High          Low  \\\n","count  5255.000000  5255.000000  5255.000000  5255.000000  5255.000000   \n","mean   2627.000000  2627.000000   158.335855   160.109968   156.388133   \n","std    1517.132163  1517.132163   161.309153   162.588024   159.766714   \n","min       0.000000     0.000000    12.990000    13.190000    12.720000   \n","25%    1313.500000  1313.500000    37.940000    38.580000    37.160000   \n","50%    2627.000000  2627.000000   106.370000   108.000000   104.880000   \n","75%    3940.500000  3940.500000   187.765000   189.405000   185.635000   \n","max    5254.000000  5254.000000   702.410000   705.070000   699.570000   \n","\n","             Close        Volume  \n","count  5255.000000  5.255000e+03  \n","mean    158.286548  2.122191e+07  \n","std     161.195754  1.824189e+07  \n","min      13.120000  7.025000e+05  \n","25%      37.980000  6.705350e+06  \n","50%     106.130000  1.677300e+07  \n","75%     187.560050  2.976013e+07  \n","max     702.100000  1.895606e+08  \n","\n","Tamaño del dataframe:\n","(5255, 8)\n","\n","Tipos de datos en cada columna:\n","index           int64\n","Unnamed: 0      int64\n","Date           object\n","Open          float64\n","High          float64\n","Low           float64\n","Close         float64\n","Volume        float64\n","dtype: object\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["# Verificar las primeras filas del dataframe\n","print(\"Primeras filas del dataframe:\")\n","print(df.head())\n","\n","# Obtener estadísticas resumidas del dataframe\n","print(\"\\nEstadísticas resumidas del dataframe:\")\n","print(df.describe())\n","\n","# Verificar el tamaño del dataframe\n","print(\"\\nTamaño del dataframe:\")\n","print(df.shape)\n","\n","# Verificar el tipo de datos en cada columna\n","print(\"\\nTipos de datos en cada columna:\")\n","print(df.dtypes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZGQJ4pnzoLn"},"outputs":[],"source":["# Definir el entorno\n","# env = DummyVecEnv([lambda: Monitor(StockTradingEnv(df))])\n","\n","#  # Inicializar el modelo\n","# model = PPO(\"MlpPolicy\", env, verbose=1)\n","\n","#  # Crear un callback de evaluación\n","# eval_env = DummyVecEnv([lambda: StockTradingEnv(df)])\n","# eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n","#                               log_path='./logs/', eval_freq=5000,\n","#                               deterministic=True, render=False)\n","\n","#  # Entrenar el modelo\n","# model.learn(total_timesteps=10000, callback=eval_callback)"]},{"cell_type":"code","source":["# # Entrenamiento del modelo\n","# model.learn(total_timesteps=20000, callback=eval_callback)\n","\n","# total_episodes = 100\n","# total_reward = 0\n","# total_trades = 0\n","\n","# for episode in range(total_episodes):\n","#     obs = eval_env.reset()\n","#     done = False\n","#     episode_reward = 0\n","#     episode_trades = 0\n","\n","#     while not done:\n","#         action, _ = model.predict(obs)\n","#         obs, rewards, done, info = eval_env.step(action)\n","#         episode_reward += np.sum(rewards)\n","#         episode_trades += np.sum([1 for r in rewards if r != 0])\n","\n","#     total_reward += episode_reward\n","#     total_trades += episode_trades\n","\n","#     # Renderizar y guardar la gráfica al final de cada episodio\n","#     eval_env.envs[0].render()\n","\n","# avg_reward = total_reward / total_episodes\n","# avg_trades = total_trades / total_episodes\n","\n","# print(f\"Media de recompensa en {total_episodes} episodios: {avg_reward}\")\n","# print(f\"Número medio de operaciones en {total_episodes} episodios: {avg_trades}\")"],"metadata":{"id":"UQrK4Rmp5fMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"U5vxEmume30l"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1SO2OY3OQsOGy66tivIsUowiO6p6SCKhM","authorship_tag":"ABX9TyMSBqxVGOyqa6YcwRMiUKDq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}